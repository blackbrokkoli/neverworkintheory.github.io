thank you for having me here so this is work that i did with uh my master's student uh reza and my postdoc hema who's now a professor at ubc okanagan so we wanted to look at bias when people are evaluating code contribution in open source software and if there is bias what the population of the people contributing in open source software is and so i think i'm gonna just go uh so open source software often thinks of itself as a meritocracy and oftentimes it is but uh and and they think that you know the quality of the contribution is the key here and it doesn't matter who is contributing where they're contributing from uh and uh that's that's that's a popular uh our past research has kind of shown that there are caveats to this they've found that

gender has a role to play when open source contributions are being evaluated uh research has also shown that uh contributions from different countries may have uh different likelihoods of their contributions getting accepted but one thing that we didn't quite see is race and and there is some evidence that uh developers do understand the race and ethnicity of other members in their open source projects even if they have not met them even if it's completely remote right today

company or industrial research industrial development happens remotely but open source software has been happening remotely for decades now right and even in that case they kind of are aware of the ethnicity of the members in their team and in that survey they also found that about 30 percent of them have uh faced some kind of negative experience due to their uh diversity and so what we wanted to see was we wanted to see if the uh

ethnicity of the person making the contribution has any impact on whether their contribution is going to be accepted and what we think is that by knowing that ethnicity by looking at a name something in my brain gets activated and some bias that i might have towards that race or ethnicity might kick in and i might view that contribution as something not great right in this case just looking at my name and saying oh this looks like a south asian contributor so this is going to be a good one or something like that and then uh accepting the contribution so what we wanted to see was

collect quantitative evidence of whether this exists or not so we took about 46 000 projects from github all of which had at least 10 stars and they were non-trivial in some sense they were not like student projects we got about 2.5 million pull requests from these and we took the names of the people who made these contributions and using a tool called name prism we kind of extracted the race and ethnicity from the name so we got the race and ethnicity for about 493 000 uh developers so one might so i mean the the tool kind of gives an output and says that this name sounds hispanic with a 97 probability or this percent this name sounds uh white with a 51 percent probability now you may think hey there's going to be problems here and there are going to be problems here uh you know a very john doe might be a name and you wouldn't know which race or ethnicity that might be you might think that it's actually a white uh person even if it may not be but we found that whenever the tool makes mistakes about someone's racial ethnicity humans make the same mistake about the race or ethnicity of that person as well so the tool is about as good as humans in determining what the race or ethnicity of a person is from the name so we took that in first striking result that we got was that less than 10 of the contributions that we were able to identify uh came from a non-white developer and that includes asians hispanic and

black developers put together right we found one alaskan or native american developer in the whole data set which is in itself we could have stopped the study here and said you know what this is this is a bad enough like less than 10 of the contributions are coming from non-white developers um or perceived non-white developers we wanted to see given this smaller population is there an even further impact on whether their contributions are being accepted or not and so what we did is we collected a whole set of other metrics other than just their race or ethnicity like you know their experience uh have they

how long have they been working in that particular project how many files have they changed a lot of other variables and we built this regression model to find out whether their contribution would be accepted or not can we predict whether someone's contribution be accepted accepted or not and find the likelihood of their contribution being accepted what we did find is that there is a relationship between someone's race or ethnicity from their names and whether their contributions are going to be accepted so what we found is that hispanic developers have about six percent lower odds of getting their poll request accepted keep in mind that this is controlling for their experience and various other uh metrics as well and api developers which is asian or pacific islander have about 10 percent lower odds of getting their poll request accepted so there is a as there is strong evidence that they have that non-white people have their contributions accepted at a lower rate we also wanted to see whether this was true when taking the ethnicity of the person integrating the code as well is taken into account and we found that non-white developers are actually more likely to get their contributions accepted when the integrator is also of the same ethnicity right and to give some results when it's hispanic a hispanic developer is going to have a 75 percent higher odds of getting their full request accepted when the integrator is also estimated as hispanic and when it comes to asian pacific islander it's about 36 percent higher ads this is in comparison to when the integrator is a white developer and the most stark result is when it's a black developer and here it's not nine percent it's actually nine times higher odds so that's 900 right so this is very very uh considerable amount of uh uh considerable result here right so we know from these results that a that representation is disproportional to the population of people and that unconscious unconscious bias may exist now it may not be that someone is saying oh this person is asian and therefore i'm going to reject their full request or this person is hispanic i'm going to reject their full request but there might be other factors that someone might associate with them right the english is not great in their comment or i don't understand uh

you know the variable names that they've used or you know or that this person is not as experienced as i thought they were which is not something that we saw in the first slide we saw that it it only the contribution matters and not so much the other factors right so what now do we just do like an author blended evaluation in github just remove the name so that you don't know who it is i don't think so i think we can actually use the author names and actively support a diverse group of contributors contributing in their projects so know that this person is from a different place or ethnicity know that they might be a new user help them get that contribution accepted don't just ignore it don't just reject it even if you're going to reject it please give them constructive feedback so that the next time around they can get their full request accepted so with that i'll share some of the papers that we have this these papers have more details on the projects that we did and all the specific experimental settings all our data and scripts are available if you want to take this and run this on your own repositories or compare it you can the model scripts are also available in the papers and if you want to reach hema and my email addresses are there and my twitter handle is also here so thank you