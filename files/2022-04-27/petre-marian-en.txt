Software rarely works as intended while it's being written, things go wrong, we know that. A standard behavior in some software development is, see the bug, swat the bug, and be done with it. In contrast, one of the things that I've observed over time is that for experts error, or more broadly things that go wrong or go amiss during software development, is opportunity: to understand better, to question assumptions, to detect miscommunication or misconceptions, to stumble onto insight. So experts are not fearful of error but watchful. They often hold off swatting the bug, instead asking, that's odd, why is that? So indeed error is seen as a useful input in the course of progressive development. So this talk is meant to summarize some of the insights about how experts and high performing teams use that opportunity.
I've spent some 30 years studying experts and high performing teams - at work, in industry - in order to - to articulate their strategies and practices, and in effect I act as a mirror or a lens, reflecting and focusing. I'm most interested in articulating what successful software developers actually do, not in dictating to them what they should do. And I'm hoping that the talk will have some resonance with your experience.
Research in software engineering predominantly considers error retrospectively, based on analysis of software and operation, usually of massive projects, usually in the context of flaws left in the code that need to be fixed or of software system failures that arise from a collection of smaller flaws. We've been taking a more ecological view of error during software development.
The psychology literature offers the concept of active error: these human errors during a task that take the form of slips of action things, like typos, or lapses of memory or attention, or mistakes made in forming and executing intentions during problem solving - so bad decisions. For recovery, a person must know that an error has occurred, must identify both what was done wrong and what should have been done, and then must understand how to undo the effects of the error. So active errors can be caught in the act, or they may be detected later during standard checks and evaluation, by obstacles to progress, from cues from the environment, or through unexpected outcomes. So error detection and recovery unfold in the course of progressive problem solving. So, what is it that experts and high performing teams do that gives them better results?
Experts mind the gaps. Rather than just looking for what they expect, they pay attention to the feedback and cues that might alert them to something unexpected, something amiss. They pay attention to the spaces between things, so for example to interfaces, interactions between components, integration with other systems, domain concepts hidden behind standard data types. They pay attention to what isn't shown, to what's missing, whether from the design or from the information or from the reasoning tool that they're using, and this minding of the gaps promotes detection of flaws.
Whereas many people look for evidence that things are working as expected, experts in high performing teams are more available to contrary evidence, and indeed their practices prime them to look for it. They seek evidence, they ask why, they engage users, they de-correlate, eliciting and contrasting different perspectives. So they challenge themselves - they challenge their assumptions, their models, their designs, through mechanisms such as the skeptic in the corner or pair debugging. They seek falsification: they don't just ask, "How would I know if this is right?" but they also ask, "How would I know if this were wrong?" and "How would I would know if an alternative were right?" Importantly, they understand that code is read by people, and they write comments about what is not in the code, that is their intentions and assumptions.
Understanding something by breaking it is a form of analytic that's common in many branches of engineering - introducing errors or flaws deliberately can be a way of gaining insight into how a system operates. Experts who have experience doing that intentionally, to test their system, also see unexpected breakage as a potential analytic, and seize the opportunity to use it.
In contrast to eliminating bugs as quickly as possible, experts reflect on the problem and on the solution model. They recognize that a small bug may sink - signal something more. Rather than dismissing simple bugs as novice errors or one of those things, they look around to detect if there's a fuller story, thereby often detecting other deeper issues such as design flaws or misconceptions.
So, experts don't just fix the bug - the one bug - they stand back and look for the other bugs that hang out with it. They consider dependencies, and reflect on the code structure in order to understand whether the bug is part of a bigger picture.
And this is all part of reassessing the landscape and deliberately expanding the search space - a way of examining barriers, understanding constraints, revealing assumptions, looking beyond the immediate issues, and hence potentially admitting more potential solutions or broadening the definition of the problem in a way that provides insight and overcomes flaws. And they do this periodically throughout the design and development process, not just at the beginning. Now this is at odds with many software development methodologies which typically concern convergence to a solution. And so sometimes the high performing teams step away from a methodology. This business of standing back and reflecting on the landscape is crucial. We all know of examples where the software met the spec but the specification was inadequate.
Software developers don't work in an ideal world - we know that - but rather in an environment dominated by conflicting demands and time pressures, so bugs are understood in the context of software use. Effective triage has to do with a cost-benefit assessment of the relative impact of the bug against the cost of fixing it. Bugs that aren't important are often tolerated or deferred. Brian Randall encompasses this in his concept of dependability. His definition leaves room for imperfection in the code if the imperfection doesn't impair the software's dependability. So tolerance is about managing the bug technically, but also about managing the bug socially: leaving compilation warnings in the code as reminders, documenting the deferral, and its rationale.
Similarly, developers have been shown to compromise at times in order to keep the work moving along. Their strategies may include deliberate sub-optimal choices calculated to serve immediate needs but enabling progressive improvements. So deliberate compromising suggests that the developer is actively managing the issue over time, implementing incremental pragmatic solutions as required to advance the larger program of work. This strategy allows the developer to explore the problem over time and ultimately to find the better solution.
But in addition to this, developers have safety nets, and one of them is pair debugging. Pair debugging is something most of the high performing teams do and people don't talk about much. They sit together and talk through the code, often deliberately matching people of notionally different levels of expertise or who know different parts of the code base. And this brings a fresh perspective to the code, spreads the knowledge of the code among the team, and has a tendency to expose assumptions, misconceptions, and miscommunications.
Expert - experts reflect on their tools as well as their code. How can you verify that an analysis tool is doing what it's meant to? Well, experts play methods against each other to increase the likelihood of detections: for example, building errors into code to test the test harness. Experts address tool limitations by combining or swapping among multiple tools; to quote one developer, often it's a mishmash of different ways of thinking that gets you the answer. So multiple techniques and tools imply more ways to think, but they also require greater cognitive overheads, and that requires intelligent coordination. So the selection is not arbitrary: teams try tools, assess their merits, assemble tool kits that both fit their development culture and span different perspectives. So, in summary, experts use systematic discipline practices that are socially embedded and reinforced.
Importantly, because there is a disciplined culture, they're able to rely on the team to catch slips, thereby giving individuals the freedom to experiment. A study of high performing teams makes it clear that the interplay between developers is crucial - plays a crucial part in both nurturing creativity and innovation, and in handling errors effectively and embedding systematic practice and rigor. So the team culture which leverages both individual strengths and multiples perspectives provides the safety net.
There is a caveat to this approach to error, which is that the focus is on fixing the error rather than fixing the blame. The team culture matters - it embodies the mindset that sees error as opportunity, that embraces multiple perspectives, that reinforces practices such as triage or playing methods against each other or pair programming, that routinely challenge understanding and assumptions. This helps strengthen and develop the team as well as improving the software. But differently, software expertise doesn't happen by accident. There are - these are practices that you can understand and invest in by making space in your organizational culture and by investing time for this mindset these sorts of practices - these dialogues - you're making space for expertise to work and to grow and for expert level software development to, to become possible. So perhaps treat this as an opportunity to reflect on your practice. Thank you for listening.
