Hi everyone good afternoon, evening, or morning depending on where you are right now - super excited to be here - beyond excited to talk to you guys about some work that I have done and I'm continuing to work on that I think will be of interest - to all parties here and that is the work that I've been doing on causal testing - understanding defects' root causes.
So today specifically I'm going to talk to you all about, first, how we - and by we I mean me - how I got here to talk to you about causal testing today.
I'm going to talk to you about causal testing, which at its foundation is just a method for improving what you already do with what already exists.
I'm going to talk about other areas and ways that causal testing can be used in practice, and I'm going to talk a little bit about whether it's actually found to be useful, starting with how we got here, what's the back story, right, like how how do we even get to talking about causal testing today.
Well it all started with a study that I collaborated on ten years ago this year or next year - coming up on 10 years ago - which is absolutely outrageous to think about, but in my PhD we - at the beginning of my PhD we were really interested in getting a kind of foundational understanding of - in the space of all the tools that are available for developers, why do they use the ones they do use, and why don't they use the ones that they don't use, right?
And so this is a really fun study to run - and from that we found a few things.
So we found that some of the major issues that developers have with the tools that are available to them are around the tool output, so issues around digesting and understanding the output, more specifically understanding the results that the tool provides and answering questions like, why?
Why is this a problem?
Why should I care?
What do I do differently?
Tool design issues - which I think we can all agree - the list here probably goes on and on, but different things cited under there, and then also workflow integration tools that are - that seem awesome and maybe could be great, but require some overhead to integrate into their current processes.
And so from the study I went on a mission to provide what would be considered useful, usable, and most importantly validated as being such, and interventions for improving software practice.
So now fast forward some years to a post PhD, I ended up getting an opportunity to do a postdoc, and in that postdoc I was given the opportunity to work in the testing space, which was actually extremely exciting for me because in my PhD I spent a lot of time focused on static analysis, and really only got to touch a little bit on the dynamic analysis side of things.
So I was really excited to have this opportunity.
And of course we already know that testing is a powerful and commonly used way of assessing and validating and/or improving software quality, but some of the - a couple things that emerged, or that I got a deeper understanding of as I did this work, or at least started doing this work, was that there are a lot of testing techniques that are available for you.
Some have come from research, some are from practice, some are a nice balance of both, but there are a lot out there.
And I also noticed that traditional testing alone doesn't actually answer the question, why is this happening?
Right, it'll help us find a defect, it'll help us even locate it in our code to some extent, but it doesn't always - it almost never answers the question, why did this behavior happen?
And so from doing some of this kind of background work and reading, I came to a question of, can we take what developers are already doing and the work that's already being done, to provide insights that existing tools don't currently provide, specifically in this case, helping answer the why question - why is this happening?
And so to this question we provided a possible solution that we - I'll talk more in depth about, in terms of why it is a solution, and that would be causal testing.
So this is where causal testing comes in, right, so causal testing is a method for conducting automated causal experiments.
And this process starts with your existing test cases and uses existing debugging techniques such as fuzzing and automated test generation, with a goal of providing developers with minimally-different passing and failing executions that help reason about why that failing behavior happened to begin with.
So how does causal testing do that?
How does that work?
Let's dig a little deeper and talk about the - the process of using causal testing.
Right, so say you have a test suite and maybe you have some continuous integration set up or something like that where a test fails, and you get notified of it.
So say you have this notification or bug report that comes up: directions from this location to that location are wrong, right, and so let's say we got that bug report because this specific test failed.
So what causal testing does is, it takes this failing test and it takes the inputs from this failing test and it attempts to perturb them in meaningful ways to produce additional valid tests that we can execute and determine - and keep track of whether they are passing or they're failing.
Once we have a set of passing and failing tests, causal testing compares these tests to the original using both the input to the test as well as the execution path that it takes in order to present the developer with the most similar tests, assuming that that means that these are the most relevant to that original failing execution.
And so in this example, given these similar passing and failing tests, we pretty quickly are able to determine that our passing tests are starting and ending in the same country, whereas our failing tests are starting and ending in different countries.
And so now we have a better understanding with minimal effort of why this test failed to now go and address it.
So you might be thinking - I'm hoping you're thinking - wow, that's, like, so simple and so cool.
I know, it got me excited too.
And you might also be thinking, what else can we do with this: also what I'm thinking.
So let's talk about it.
What else can causal testing do?
Is it a one-trick pony or can it be applied other places?
Well a couple of directions that we're looking at are, first, causal fairness testing, and so - and this work we actually have published as a demo and there is a prototype that has been developed at the link provided here, but, so, causal fairness testing takes this causal experimentation approach in the context of detecting bias.
So let's say, for example, we have some software and that software takes some inputs.
For simplicity's sake, yeah, let's say it's some loan software that takes these four inputs to make a decision.
What causal fairness testing does is, it automatically generates tests that look something like this.
We have some input based on our input space, it goes into the loan software, and we observe, what is the outcome of that input based on that test.
Causal testing makes small singular changes to the input,
so for example changing green Brittany's race to orange Brittany, right, and conducts the same test where we feed it into the software and observe the outcome - flip one additional attribute - one singular attribute - observe the outcome.
And we do that over and over and over again within some threshold to help answer questions, such as, how often is the outcome of my software different just because of race, right.
So providing a method for - for kind of - if you're worried about software that may have liability concerns or accountability concerns around bias or fairness, providing a method for you to not have to create those tests on your own - to be able to automatically generate tests, that can help speak to those types of concerns.
So that's one space where causal fairness testing could be useful, or causal testing.
Another space that we're looking at that I think is really important to - to really dig into - is this idea of testing machine learning based software.
And so the work we're doing right now is looking at - so for example, say you have some software and that software integrates some trained machine learning model that aids in the decision making, right, and into that software, or some sets of inputs let's say for this software we care about name, race, zip code, and the degree that they have, right, and then presumably there is either some concrete set of outputs or classes of outputs.
Since we are using a machine learning model here, we want to make sure our software is - is - is complying with - with respect to our expectations, right, so what we're starting right now is, what does it look like to test this type of software, particularly in the mode of testing that we typically use - that being assertions, right, so can we write assertions that look something like this where we assert equal output or outcomes for two sets of inputs, and then another example asserting true that for some input we end up in a class or some specific output.
And if this is something that we can do, then we can actually start to think about causal testing being beneficial in this context as well for us to be able to - for example, see that if we change April to Adam, right, our assertion doesn't break, versus here it is breaking, right.
If we keep doing that and we get enough tests then we can start to reason about why something about this input space is causing unexpected behavior, all right, just another step in the information chain that's required not only to understand behavior but to actually rectify it.
So two directions, super excited about we're working on in our lab right now, but you might be of course wondering - what you should be wondering - is it actually useful? Can I take this technique and do something meaningful with it in practice?
And we developed a proof of concept implementation to evaluate exactly this, where we found that in terms of improving the ability to detect root cause - fixing these defects and being useful - causal testing checks all the boxes, and more specifically in terms of being useful, these similar passing tests point to the cause in terms of our - according to our participants.
And so in summary causal testing is a useful technique that provides more insight into faulty executions with code that you've already written.
So don't hesitate look into the work, talk to me about it, let's figure out how causal testing can become a part of your testing process.
Thank you so much for your time.
